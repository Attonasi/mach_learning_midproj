{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Write Up\n",
    "\n",
    "### Goals\n",
    "\n",
    "#### My goals with this project were to get familiarity with as much of the sklearn library as possible while fulfilling the assignment. \n",
    "\n",
    "#### My approach to building a srong predictor is not so much intuitive as it is thourough. I don't know which ones will work best. So  I will try as many as I can.  Given these tools it takes me less time to write several for loops and try different combinations than it does "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you put this in edit mode you will see a list of shortcuts. \n",
    "\n",
    "Enter           enter edit mode                 Shift-­Enter     run cell, select below\n",
    "Ctrl-Enter      run cell                        Alt-Enter       run cell, insert below\n",
    "Y               to code                         M               to markdown\n",
    "R               to raw                          1               to heading 1\n",
    "2,3,4,5,6       to heading 2,3,4,5,6            Up/K            select cell above\n",
    "Down/J          select cell below               A/B             insert cell above/­below\n",
    "X               cut selected cell               C               copy selected cell\n",
    "Shift-V         paste cell above                V               paste cell below\n",
    "Z               undo last cell deletion         D,D             delete selected cell\n",
    "Shift-M         merge cell below                Ctrl-S          Save and Checkpoint\n",
    "L               toggle line numbers             O               toggle output\n",
    "Shift-O         toggle output scrolling         Esc             close pager\n",
    "H               show keyboard shortcut help     I,I             interrupt kernel\n",
    "0,0             restart kernel                  Space           scroll down\n",
    "Shift-­Space    scroll up                       Shift           ignore\n",
    "\n",
    "Edit Mode (press Enter to enable)\n",
    "Tab                 code completion or indent   Shift-Tab           tooltip\n",
    "Ctrl-]              indent                      Ctrl-[              dedent\n",
    "Ctrl-A              select all                  Ctrl-Z              undo\n",
    "Ctrl-S­hift-Z       redo                        Ctrl-Y              redo\n",
    "Ctrl-Home           go to cell start            Ctrl-Up             go to cell start\n",
    "Ctrl-End            go to cell end              Ctrl-Down           go to cell end\n",
    "Ctrl-Left           go one word left            Ctrl-Right          go one word right\n",
    "Ctrl-B­ack­space    delete word before          Ctrl-D­elete         delete word after\n",
    "Esc                 command mode                Ctrl-M              command mode\n",
    "Shift-­Enter        run cell, select below      Ctrl-Enter          run cell\n",
    "Alt-Enter           run cell, insert below      Ctrl-S­hif­t-S­ubtract split cell\n",
    "Ctrl-S­hift--       split cell                  Ctrl-S              Save and Checkpoint\n",
    "Ctrl-/              toggle comment on current or selected lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/collin/anaconda3/lib/python3.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/collin/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ClassifierMixin\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "path = '/home/collin/machine_learning/CS6140ML18Fall-smith.colin/midterm/data/'\n",
    "data_file = 'weatherAUS.csv' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path + data_file, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 145460 entries, 0 to 145459\n",
      "Data columns (total 24 columns):\n",
      "Date             145460 non-null object\n",
      "Location         145460 non-null object\n",
      "MinTemp          143975 non-null float64\n",
      "MaxTemp          144199 non-null float64\n",
      "Rainfall         142199 non-null float64\n",
      "Evaporation      82670 non-null float64\n",
      "Sunshine         75625 non-null float64\n",
      "WindGustDir      135134 non-null object\n",
      "WindGustSpeed    135197 non-null float64\n",
      "WindDir9am       134894 non-null object\n",
      "WindDir3pm       141232 non-null object\n",
      "WindSpeed9am     143693 non-null float64\n",
      "WindSpeed3pm     142398 non-null float64\n",
      "Humidity9am      142806 non-null float64\n",
      "Humidity3pm      140953 non-null float64\n",
      "Pressure9am      130395 non-null float64\n",
      "Pressure3pm      130432 non-null float64\n",
      "Cloud9am         89572 non-null float64\n",
      "Cloud3pm         86102 non-null float64\n",
      "Temp9am          143693 non-null float64\n",
      "Temp3pm          141851 non-null float64\n",
      "RainToday        142199 non-null object\n",
      "RISK_MM          142193 non-null float64\n",
      "RainTomorrow     142193 non-null object\n",
      "dtypes: float64(17), object(7)\n",
      "memory usage: 26.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindSpeed9am</th>\n",
       "      <th>WindSpeed3pm</th>\n",
       "      <th>Humidity9am</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RISK_MM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>143975.000000</td>\n",
       "      <td>144199.000000</td>\n",
       "      <td>142199.000000</td>\n",
       "      <td>82670.000000</td>\n",
       "      <td>75625.000000</td>\n",
       "      <td>135197.000000</td>\n",
       "      <td>143693.000000</td>\n",
       "      <td>142398.000000</td>\n",
       "      <td>142806.000000</td>\n",
       "      <td>140953.000000</td>\n",
       "      <td>130395.00000</td>\n",
       "      <td>130432.000000</td>\n",
       "      <td>89572.000000</td>\n",
       "      <td>86102.000000</td>\n",
       "      <td>143693.000000</td>\n",
       "      <td>141851.00000</td>\n",
       "      <td>142193.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.194034</td>\n",
       "      <td>23.221348</td>\n",
       "      <td>2.360918</td>\n",
       "      <td>5.468232</td>\n",
       "      <td>7.611178</td>\n",
       "      <td>40.035230</td>\n",
       "      <td>14.043426</td>\n",
       "      <td>18.662657</td>\n",
       "      <td>68.880831</td>\n",
       "      <td>51.539116</td>\n",
       "      <td>1017.64994</td>\n",
       "      <td>1015.255889</td>\n",
       "      <td>4.447461</td>\n",
       "      <td>4.509930</td>\n",
       "      <td>16.990631</td>\n",
       "      <td>21.68339</td>\n",
       "      <td>2.360682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.398495</td>\n",
       "      <td>7.119049</td>\n",
       "      <td>8.478060</td>\n",
       "      <td>4.193704</td>\n",
       "      <td>3.785483</td>\n",
       "      <td>13.607062</td>\n",
       "      <td>8.915375</td>\n",
       "      <td>8.809800</td>\n",
       "      <td>19.029164</td>\n",
       "      <td>20.795902</td>\n",
       "      <td>7.10653</td>\n",
       "      <td>7.037414</td>\n",
       "      <td>2.887159</td>\n",
       "      <td>2.720357</td>\n",
       "      <td>6.488753</td>\n",
       "      <td>6.93665</td>\n",
       "      <td>8.477969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-8.500000</td>\n",
       "      <td>-4.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>980.50000</td>\n",
       "      <td>977.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-7.200000</td>\n",
       "      <td>-5.40000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.600000</td>\n",
       "      <td>17.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>1012.90000</td>\n",
       "      <td>1010.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>12.300000</td>\n",
       "      <td>16.60000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>22.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>1017.60000</td>\n",
       "      <td>1015.200000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>16.700000</td>\n",
       "      <td>21.10000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16.900000</td>\n",
       "      <td>28.200000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>1022.40000</td>\n",
       "      <td>1020.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>21.600000</td>\n",
       "      <td>26.40000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>33.900000</td>\n",
       "      <td>48.100000</td>\n",
       "      <td>371.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1041.00000</td>\n",
       "      <td>1039.600000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>40.200000</td>\n",
       "      <td>46.70000</td>\n",
       "      <td>371.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             MinTemp        MaxTemp       Rainfall   Evaporation  \\\n",
       "count  143975.000000  144199.000000  142199.000000  82670.000000   \n",
       "mean       12.194034      23.221348       2.360918      5.468232   \n",
       "std         6.398495       7.119049       8.478060      4.193704   \n",
       "min        -8.500000      -4.800000       0.000000      0.000000   \n",
       "25%         7.600000      17.900000       0.000000      2.600000   \n",
       "50%        12.000000      22.600000       0.000000      4.800000   \n",
       "75%        16.900000      28.200000       0.800000      7.400000   \n",
       "max        33.900000      48.100000     371.000000    145.000000   \n",
       "\n",
       "           Sunshine  WindGustSpeed   WindSpeed9am   WindSpeed3pm  \\\n",
       "count  75625.000000  135197.000000  143693.000000  142398.000000   \n",
       "mean       7.611178      40.035230      14.043426      18.662657   \n",
       "std        3.785483      13.607062       8.915375       8.809800   \n",
       "min        0.000000       6.000000       0.000000       0.000000   \n",
       "25%        4.800000      31.000000       7.000000      13.000000   \n",
       "50%        8.400000      39.000000      13.000000      19.000000   \n",
       "75%       10.600000      48.000000      19.000000      24.000000   \n",
       "max       14.500000     135.000000     130.000000      87.000000   \n",
       "\n",
       "         Humidity9am    Humidity3pm   Pressure9am    Pressure3pm  \\\n",
       "count  142806.000000  140953.000000  130395.00000  130432.000000   \n",
       "mean       68.880831      51.539116    1017.64994    1015.255889   \n",
       "std        19.029164      20.795902       7.10653       7.037414   \n",
       "min         0.000000       0.000000     980.50000     977.100000   \n",
       "25%        57.000000      37.000000    1012.90000    1010.400000   \n",
       "50%        70.000000      52.000000    1017.60000    1015.200000   \n",
       "75%        83.000000      66.000000    1022.40000    1020.000000   \n",
       "max       100.000000     100.000000    1041.00000    1039.600000   \n",
       "\n",
       "           Cloud9am      Cloud3pm        Temp9am       Temp3pm        RISK_MM  \n",
       "count  89572.000000  86102.000000  143693.000000  141851.00000  142193.000000  \n",
       "mean       4.447461      4.509930      16.990631      21.68339       2.360682  \n",
       "std        2.887159      2.720357       6.488753       6.93665       8.477969  \n",
       "min        0.000000      0.000000      -7.200000      -5.40000       0.000000  \n",
       "25%        1.000000      2.000000      12.300000      16.60000       0.000000  \n",
       "50%        5.000000      5.000000      16.700000      21.10000       0.000000  \n",
       "75%        7.000000      7.000000      21.600000      26.40000       0.800000  \n",
       "max        9.000000      9.000000      40.200000      46.70000     371.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Data Cleaning and manipulation\n",
    "\n",
    "Because the data in this data set is such a mess it needs to be cleaned up. In order to get the best results I am going to make about a half a dozen datasets and see which is most predictive. There are so many issues with this pile of data. I am going to start with a list of hypotheses. \n",
    "\n",
    "1. Wind direction matters, but it will matter \"differently\" in different locations. The Ocean will be in a different direction in diffrent places. If you just use one dataset my guess is wind direction will look like noise. Separated by loction it will be useful. All of these things are going to be different in different locations. The main dataset will have a predictor. But to the extent it is feasible each location must be dealt with individually. Humidiy variation will have different predictive value inland than it will in coastal areas.\n",
    "\n",
    "2. This is a dirty dataset. Variations need to be made for each column replacing NaNs with 0, mean, min and max and the combinations run against eachof the variations of hte other columns to be thorough as well as variations that drop each column. The cloumns that are being 1 hot encoded can ignore them. \n",
    "\n",
    "3. The dates are going to matter by season. I don't know what the seasons are in australia so month seems like the most likely step. Again twelve new one hot columns minus the date column. This will have to be done before cleaning up sunshine and evaporation. \n",
    "\n",
    "4. Sunshine and Evaporation are sensitive to the time of year and by location. But I think more by time of year. The NaN values will be replaced by the average value for that month. It is going to take some effort to find the averages by month for each but the  columns may as well be dropped otherwise. I would like to do this by location as well, but there are 46 of those and 12 months. That would be 552 combinations. =/ \n",
    "\n",
    "First step is to turn the date into the month.\n",
    "\n",
    "Then I will turn the object columns into 1 hot columns with get_dummies.\n",
    "\n",
    "data_y will be set to RainTomorrow.\n",
    "\n",
    "Then I will drop the silly columns including the two answer columns. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_arr = data['Date']\n",
    "\n",
    "months = []\n",
    "for line in date_arr:\n",
    "    months.append(line[5] + line[6])\n",
    "    \n",
    "data_months = pd.Series(months)\n",
    "data_months.name = 'Month'\n",
    "    \n",
    "data = pd.concat([data, data_months], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gust_dummies = pd.get_dummies(data['WindGustDir'], prefix='gust_')\n",
    "nine_am_dummies = pd.get_dummies(data['WindDir9am'], prefix='9am_')\n",
    "three_pm_dummies = pd.get_dummies(data['WindDir3pm'], prefix='3pm_')\n",
    "location_dummies = pd.get_dummies(data['Location'])\n",
    "month_dummies = pd.get_dummies(data['Month'], prefix='month:')\n",
    "rain_today_dummies = pd.get_dummies(data['RainToday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_dummies = pd.concat([data, gust_dummies, nine_am_dummies, three_pm_dummies, location_dummies, month_dummies, rain_today_dummies], axis=1)\n",
    "\n",
    "data_with_dummies['RainTomorrow'].replace(['No', 'Yes', 'NaN'], [0, 1, 0], inplace=True)\n",
    "\n",
    "data_y = data_with_dummies[\"RainTomorrow\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data_with_dummies.drop(columns=\n",
    "    ['Date', 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'Month', 'RainToday', 'RISK_MM'])\n",
    "\n",
    "cols = data_clean.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = Imputer(missing_values='NaN', strategy='mean', copy=False)\n",
    "\n",
    "imp.fit(data_clean, data_y)\n",
    "imputed = imp.transform(data_clean)\n",
    "data_clean = pd.DataFrame(imputed)\n",
    "\n",
    "data_clean.columns = cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data above has been dummied and the nan values inputed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y = data_clean['RainTomorrow']\n",
    "\n",
    "data_fin = data_clean.drop(columns=['RainTomorrow'])\n",
    "\n",
    "data_fin_clip = data_fin.clip(lower=0)\n",
    "\n",
    "for i in range (len(data_y)):\n",
    "    if data_y[i] < 1:\n",
    "        data_y[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "cols = data_fin_clip.columns.tolist()\n",
    "datax = pd.DataFrame(scaler.fit_transform(data_fin_clip))\n",
    "datax.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_data = pd.DataFrame(normalize(data_fin_clip))\n",
    "l1_data = pd.DataFrame(normalize(data_fin_clip, norm='l1'))\n",
    "max_data = pd.DataFrame(normalize(data_fin_clip, norm='max'))\n",
    "\n",
    "l2x_data = pd.DataFrame(normalize(datax))\n",
    "l1x_data = pd.DataFrame(normalize(datax, norm='l1'))\n",
    "maxx_data = pd.DataFrame(normalize(datax, norm='max'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting a baseline. For this I am just checking to see if The data is skewed. It is. \n",
    "\n",
    "### Any effective predictor has to improve on .7808"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 0.7808332187542967\n"
     ]
    }
   ],
   "source": [
    "# Set Baseline\n",
    "\n",
    "yes_arr = rain_today_dummies['Yes']\n",
    "yes = 0\n",
    "no = 0\n",
    "for val in yes_arr:\n",
    "    if val:\n",
    "        yes += 1\n",
    "    else:\n",
    "        no += 1\n",
    "\n",
    "if no > yes:\n",
    "    this = no \n",
    "else:\n",
    "    this = yes\n",
    "\n",
    "baseline = this / (no + yes)\n",
    "print('Baseline: ' + str(baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data is \"Clean\"\n",
    "\n",
    "127  columns dtype: float\n",
    "\n",
    "data_y is set to 0's and 1's. \n",
    "\n",
    "Now we can apply some fancy stuff!\n",
    "\n",
    "For this dataset I am going to set up a list of training test splits. As I create new test splits for manipulated data sets I will add them to this list. Once the tests snf clssifiers are built ading a new normalised or regularized dataset will be easy to feed into them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 145460 entries, 0 to 145459\n",
      "Columns: 127 entries, MinTemp to Yes\n",
      "dtypes: float64(127)\n",
      "memory usage: 140.9 MB\n"
     ]
    }
   ],
   "source": [
    "data_fin_clip.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 145460 entries, 0 to 145459\n",
      "Columns: 127 entries, MinTemp to Yes\n",
      "dtypes: float64(127)\n",
      "memory usage: 140.9 MB\n"
     ]
    }
   ],
   "source": [
    "datax.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The blocs below set up  a system by which the train test sets are put in lists and more easily used later on. Each data set has a training and test split which has 4 groups. This allows me to access them as one variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sets = []\n",
    "tx_sets = []\n",
    "\n",
    "TEST_DESC = 0\n",
    "X_TRAIN = 1\n",
    "Y_TRAIN = 2\n",
    "X_TEST = 3\n",
    "Y_TEST = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP = 0\n",
    "\n",
    "basic=[]\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_fin_clip, data_y, test_size=.2, random_state=4)\n",
    "basic.append('Basic DSet: ')\n",
    "basic.append(x_train)\n",
    "basic.append(y_train)\n",
    "basic.append(x_test)\n",
    "basic.append(y_test)\n",
    "\n",
    "list=[]\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_fin_clip, data_y, test_size=.2, random_state=4)\n",
    "list.append('Clipped Dset: ')\n",
    "list.append(x_train)\n",
    "list.append(y_train)\n",
    "list.append(x_test)\n",
    "list.append(y_test)\n",
    "t_sets.append(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because this is a binary classifier I decided to try the natural binary classifiers in hte Naive-Bayes library.\n",
    "\n",
    "### Spoiler: They were all terrible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bern = BernoulliNB()\n",
    "mult = MultinomialNB()\n",
    "gaus = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bern_pred = bern.fit(basic[X_TRAIN], basic[Y_TRAIN]).predict(basic[X_TEST])\n",
    "gaus_pred = gaus.fit(basic[X_TRAIN], basic[Y_TRAIN]).predict(basic[X_TEST])\n",
    "# mult_pred = mult.fit(t_sets[BAS][X_TRAIN], t_sets[BAS][Y_TRAIN]).predict(t_sets[BAS][X_TEST])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7577340849718136\n",
      "0.6582221916678125\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(bern_pred, t_sets[0][Y_TEST]))\n",
    "print(accuracy_score(gaus_pred, t_sets[0][Y_TEST]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bern_pred = bern.fit(t_sets[CLIP][X_TRAIN], t_sets[CLIP][Y_TRAIN]).predict(t_sets[CLIP][X_TEST])\n",
    "gaus_pred = gaus.fit(t_sets[CLIP][X_TRAIN], t_sets[CLIP][Y_TRAIN]).predict(t_sets[CLIP][X_TEST])\n",
    "mult_pred = mult.fit(t_sets[CLIP][X_TRAIN], t_sets[CLIP][Y_TRAIN]).predict(t_sets[CLIP][X_TEST])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7577340849718136\n",
      "0.6582221916678125\n",
      "0.7897703836106146\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(bern_pred, t_sets[CLIP][Y_TEST]))\n",
    "print(accuracy_score(gaus_pred, t_sets[CLIP][Y_TEST]))\n",
    "print(accuracy_score(mult_pred, t_sets[CLIP][Y_TEST]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I prefer to build functions instead of copy pasting. I prefer to put these in a library and import them rather than plugging up my notebook. This works when you have a functioning function.  If you are changing and writing functions as you go they don't update when you save them. Even if you clear everything and re-import it doesn't work. \n",
    "\n",
    "### If you are udating a function you are using put it in your notebook. I will keep a library of tested and working functions and add anything new I write to it after I am done with a project.\n",
    "\n",
    "### append_tts was great here after I closed everything and restarted the computer though. It can be further abstracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funkys_funcs import append_tts\n",
    "\n",
    "a,b,c,d = train_test_split(l2_data, data_y, test_size=.2, random_state=4)\n",
    "L2 = append_tts(a,b,c,d, 'L2 Norm Clip:    ', t_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d = train_test_split(l1_data, data_y, test_size=.2, random_state=4)\n",
    "L1 = append_tts(a,b,c,d, 'L1 Norm Clip:    ', t_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d = train_test_split(max_data, data_y, test_size=.2, random_state=4)\n",
    "MAX = append_tts(a,b,c,d, 'MAX Norm Clip:   ', t_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d = train_test_split(datax, data_y, test_size=.2, random_state=4)\n",
    "DATAX = append_tts(a,b,c,d, 'DataX:         ', tx_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d = train_test_split(l2x_data, data_y, test_size=.2, random_state=4)\n",
    "L2X = append_tts(a,b,c,d, 'L2X Norm Clip:   ', tx_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d = train_test_split(l1x_data, data_y, test_size=.2, random_state=4)\n",
    "L1X = append_tts(a,b,c,d, 'L1X Norm Clip:   ', tx_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d = train_test_split(maxx_data, data_y, test_size=.2, random_state=4)\n",
    "MAXX = append_tts(a,b,c,d, 'MAXX Norm:      ', tx_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I tried the BernoulliNB on base data and all of the normalized options. I actually did this with all three classifiers. Normalizing has no affect on results. Given the poor performance I am not really sure what would improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for DataX:         0.7806613501993676\n",
      "Score for L2X Norm Clip:   0.7806613501993676\n",
      "Score for L1X Norm Clip:   0.7806613501993676\n",
      "Score for MAXX Norm:      0.7806613501993676\n"
     ]
    }
   ],
   "source": [
    "bern = BernoulliNB(alpha=2)\n",
    "\n",
    "for l in tx_sets:\n",
    "    pred = bern.fit(l[X_TRAIN], l[Y_TRAIN]).predict(l[X_TEST])\n",
    "    print('Score for ' + l[TEST_DESC] + str(accuracy_score(pred, l[Y_TEST])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To look for better results I tried many options. I did the below application with all three classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB(alpha=9.5367431640625e-07, binarize=0.0, class_prior=None,\n",
      "      fit_prior=True)\n",
      "Score for MAXX Norm:      0.7641963426371511\n",
      "BernoulliNB(alpha=1.9073486328125e-06, binarize=0.0, class_prior=None,\n",
      "      fit_prior=True)\n",
      "Score for MAXX Norm:      0.7642650900591228\n",
      "BernoulliNB(alpha=3.814697265625e-06, binarize=0.0, class_prior=None,\n",
      "      fit_prior=True)\n",
      "Score for MAXX Norm:      0.7646775745909529\n",
      "BernoulliNB(alpha=7.62939453125e-06, binarize=0.0, class_prior=None,\n",
      "      fit_prior=True)\n",
      "Score for MAXX Norm:      0.7647119483019387\n",
      "BernoulliNB(alpha=1.52587890625e-05, binarize=0.0, class_prior=None,\n",
      "      fit_prior=True)\n",
      "Score for MAXX Norm:      0.7649525642788395\n",
      "BernoulliNB(alpha=3.0517578125e-05, binarize=0.0, class_prior=None,\n",
      "      fit_prior=True)\n",
      "Score for MAXX Norm:      0.7650213117008112\n",
      "BernoulliNB(alpha=6.103515625e-05, binarize=0.0, class_prior=None,\n",
      "      fit_prior=True)\n",
      "Score for MAXX Norm:      0.7650213117008112\n",
      "BernoulliNB(alpha=0.0001220703125, binarize=0.0, class_prior=None,\n",
      "      fit_prior=True)\n",
      "Score for MAXX Norm:      0.7646088271689812\n",
      "BernoulliNB(alpha=0.000244140625, binarize=0.0, class_prior=None,\n",
      "      fit_prior=True)\n",
      "Score for MAXX Norm:      0.764230716348137\n",
      "BernoulliNB(alpha=0.00048828125, binarize=0.0, class_prior=None,\n",
      "      fit_prior=True)\n",
      "Score for MAXX Norm:      0.7643682111920803\n",
      "BernoulliNB(alpha=0.0009765625, binarize=0.0, class_prior=None,\n",
      "      fit_prior=True)\n",
      "Score for MAXX Norm:      0.7639557266602502\n",
      "BernoulliNB(alpha=0.001953125, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Score for MAXX Norm:      0.7631307575965901\n",
      "BernoulliNB(alpha=0.00390625, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Score for MAXX Norm:      0.7609308400934965\n",
      "BernoulliNB(alpha=0.0078125, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Score for MAXX Norm:      0.7569778633301251\n",
      "BernoulliNB(alpha=0.015625, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Score for MAXX Norm:      0.7521655437921078\n",
      "BernoulliNB(alpha=0.03125, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Score for MAXX Norm:      0.7470782345662038\n",
      "BernoulliNB(alpha=0.0625, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Score for MAXX Norm:      0.738553554241716\n",
      "BernoulliNB(alpha=0.125, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Score for MAXX Norm:      0.7256977863330125\n",
      "BernoulliNB(alpha=0.25, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Score for MAXX Norm:      0.709232778770796\n",
      "BernoulliNB(alpha=0.5, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Score for MAXX Norm:      0.6841055960401485\n",
      "BernoulliNB(alpha=1, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Score for MAXX Norm:      0.6593221504193593\n",
      "BernoulliNB(alpha=2, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Score for MAXX Norm:      0.6370136119895504\n",
      "BernoulliNB(alpha=4, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Score for MAXX Norm:      0.6153581740684725\n",
      "BernoulliNB(alpha=8, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Score for MAXX Norm:      0.5916059397772584\n",
      "BernoulliNB(alpha=16, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Score for MAXX Norm:      0.5633163756359136\n",
      "BernoulliNB(alpha=32, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Score for MAXX Norm:      0.5277052110545855\n",
      "BernoulliNB(alpha=64, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Score for MAXX Norm:      0.4799257527842706\n",
      "BernoulliNB(alpha=128, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Score for MAXX Norm:      0.419771758559054\n",
      "BernoulliNB(alpha=256, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Score for MAXX Norm:      0.3598240065997525\n",
      "BernoulliNB(alpha=512, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Score for MAXX Norm:      0.3138319813007012\n",
      "BernoulliNB(alpha=1024, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Score for MAXX Norm:      0.3031417571841056\n",
      "BernoulliNB(alpha=2048, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Score for MAXX Norm:      0.3619895503918603\n",
      "BernoulliNB(alpha=4096, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Score for MAXX Norm:      0.5894060222741647\n",
      "BernoulliNB(alpha=8192, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Score for MAXX Norm:      0.7775677162106421\n",
      "BernoulliNB(alpha=16384, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Score for MAXX Norm:      0.7815206929740135\n",
      "BernoulliNB(alpha=32768, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Score for MAXX Norm:      0.7815206929740135\n",
      "BernoulliNB(alpha=65536, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Score for MAXX Norm:      0.7815206929740135\n",
      "BernoulliNB(alpha=131072, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Score for MAXX Norm:      0.7815206929740135\n",
      "BernoulliNB(alpha=262144, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Score for MAXX Norm:      0.7815206929740135\n",
      "BernoulliNB(alpha=524288, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Score for MAXX Norm:      0.7815206929740135\n"
     ]
    }
   ],
   "source": [
    "for pow in range (-20, 20):\n",
    "    bern = BernoulliNB(alpha=2**pow)\n",
    "    print(bern)\n",
    "    pred = bern.fit(t_sets[1][X_TRAIN], t_sets[1][Y_TRAIN]).predict(l[X_TEST])\n",
    "    print('Score for ' + l[TEST_DESC] + str(accuracy_score(pred, t_sets[1][Y_TEST])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli alpha= 2^11\n",
    "Score for Clipped Dset: 0.7827581465695036\n",
    "Score for L2 Norm Clip: 0.7827581465695036\n",
    "Score for L1 Norm Clip: 0.7827581465695036\n",
    "Score for MAX Norm Clip: 0.7827581465695036\n",
    "\n",
    "MultinomialNB(alpha=16384, class_prior=None, fit_prior=False)\n",
    "Score for Clipped Dset: 0.7815206929740135\n",
    "Score for L2 Norm Clip: 0.7815206929740135\n",
    "Score for L1 Norm Clip: 0.7815206929740135\n",
    "Score for MAX Norm Clip: 0.7815206929740135"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With a baseline of .7808 none of these are any good. \n",
    "\n",
    "### .7897 was the best result with base multinomial on the most basic clipped dataset and an odd Alpha value\n",
    "\n",
    "### Discussing the warp and weft of these results will be more useful than me typing up the random thoughts that are driving this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7897703836106146\n"
     ]
    }
   ],
   "source": [
    "mult_pred = mult.fit(t_sets[CLIP][X_TRAIN], t_sets[CLIP][Y_TRAIN]).predict(t_sets[CLIP][X_TEST])\n",
    "print(accuracy_score(mult_pred, t_sets[CLIP][Y_TEST]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Trees\n",
    "### GradiantClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "\n",
    "pred = gbc.fit(t_sets[1][X_TRAIN], t_sets[1][Y_TRAIN]).predict(t_sets[1][X_TEST])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8499243778358312\n"
     ]
    }
   ],
   "source": [
    " print(gbc.score(t_sets[1][X_TEST], t_sets[1][Y_TEST]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now this shows promise. Lots of options and several more datasets to try. \n",
    "\n",
    "### First I want to get a single Gradient Boosting Regressor working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=1, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, presort='auto', random_state=0,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0, loss='ls')\n",
    "grb.fit(t_sets[1][X_TRAIN], t_sets[1][Y_TRAIN])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because this is a regressor it gives me a predicted value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grb.predict(t_sets[1][X_TEST])\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] < .5:\n",
    "        y_pred[i] = 0\n",
    "    else:\n",
    "        y_pred[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from result_lib import Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = Result(y_pred, t_sets[1][Y_TEST].values, .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range: 0.1\n",
      "Correct: 24433\n",
      "Mean Variance: 0.16014711948301938\n"
     ]
    }
   ],
   "source": [
    "res.display_totals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8398528805169806"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "24433/len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .8398 isn't bad, going to try changing the cut off on 1 and 0 from .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8329437646088271\n"
     ]
    }
   ],
   "source": [
    "grb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0, loss='ls')\n",
    "y_pred = grb.fit(t_sets[1][X_TRAIN], t_sets[1][Y_TRAIN]).predict(t_sets[1][X_TEST])\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] < .6:\n",
    "        y_pred[i] = 0\n",
    "    else:\n",
    "        y_pred[i] = 1\n",
    "        \n",
    "res = Result(y_pred, t_sets[1][Y_TEST].values, .1)\n",
    "\n",
    "print(res.correct/len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8390966588752922\n"
     ]
    }
   ],
   "source": [
    "grb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0, loss='ls')\n",
    "y_pred = grb.fit(t_sets[1][X_TRAIN], t_sets[1][Y_TRAIN]).predict(t_sets[1][X_TEST])\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] < .4:\n",
    "        y_pred[i] = 0\n",
    "    else:\n",
    "        y_pred[i] = 1\n",
    "        \n",
    "res = Result(y_pred, t_sets[1][Y_TEST].values, .1)\n",
    "\n",
    "print(res.correct/len(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So far best  result is .8499 whis is significantly above .7808. \n",
    "\n",
    "### Now that the basics work the next step would be to try many combinations of estimators and depth and learning rate and regressor cutoffs on the many different variations of the data I currently have.\n",
    "\n",
    "### My intuition at this point would be to work with the Gradient Boosting regressor. \n",
    "\n",
    "#### Given a baseline of .7808 I have a target improvement of 21.92%  and with minimal effort I dropped that to 15.01% The next steps would be to try the combinations in GBC of:\n",
    "\n",
    "#### alpha=0.9, \n",
    "#### criterion='friedman_mse', \n",
    "#### init=None,\n",
    "#### learning_rate=0.1,\n",
    "#### loss='ls', \n",
    "#### max_depth=1, \n",
    "#### max_features=None,\n",
    "#### max_leaf_nodes=None, \n",
    "#### min_impurity_decrease=0.0,\n",
    "#### min_impurity_split=None, \n",
    "#### min_samples_leaf=1,\n",
    "#### min_samples_split=2, \n",
    "#### min_weight_fraction_leaf=0.0,\n",
    "#### n_estimators=100, \n",
    "#### presort='auto', \n",
    "#### random_state=0,\n",
    "#### subsample=1.0, \n",
    "#### verbose=0, \n",
    "#### warm_start=False\n",
    "\n",
    "### Also Mark Down is not my friend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
